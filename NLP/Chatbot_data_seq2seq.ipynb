{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot_data_seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPml4w4SV14wmW/O4LjUnAt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParkEunHyeok/AI_Study/blob/main/NLP/Chatbot_data_seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkVx8nyPyB5L",
        "outputId": "8063981d-b0c6-4afe-d3f7-ba8e19ee8b32"
      },
      "source": [
        "!pip install konlpy\n",
        "!pip install preprocessing"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.3.0)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: preprocessing in /usr/local/lib/python3.7/dist-packages (0.1.13)\n",
            "Requirement already satisfied: nltk==3.2.4 in /usr/local/lib/python3.7/dist-packages (from preprocessing) (3.2.4)\n",
            "Requirement already satisfied: sphinx-rtd-theme==0.2.4 in /usr/local/lib/python3.7/dist-packages (from preprocessing) (0.2.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.2.4->preprocessing) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI2EF4XvQGLR"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.models import Model, load_model, save_model\n",
        "from konlpy.tag import Okt\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "from preprocessing import *"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CKOhyiWQRT9",
        "outputId": "053c39c9-f79d-48d9-cb1f-94960d2b45cd"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "path = \"gdrive/My Drive/Colab Notebooks/squad\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "G3jvwo2-WGkO",
        "outputId": "e4cb9379-0e5c-478d-b734-96e937d060b0"
      },
      "source": [
        "train = pd.read_csv(path+\"/songysData.csv\")\n",
        "train[:5]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12시 땡!</td>\n",
              "      <td>하루가 또 가네요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1지망 학교 떨어졌어</td>\n",
              "      <td>위로해 드립니다.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3박4일 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3박4일 정도 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PPL 심하네</td>\n",
              "      <td>눈살이 찌푸려지죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Q            A  label\n",
              "0           12시 땡!   하루가 또 가네요.      0\n",
              "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
              "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
              "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
              "4          PPL 심하네   눈살이 찌푸려지죠.      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ROdWWeT5C6Y"
      },
      "source": [
        "'''\n",
        " 데이터 전처리\n",
        "'''\n",
        "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
        "CHANGE_FILTER = re.compile(FILTERS) # 미리 Complie\n",
        "PAD, PAD_INDEX = \"<PAD>\", 0 # 패딩 토큰\n",
        "STD, STD_INDEX = \"<SOS>\", 1 # 시작 토큰\n",
        "END, END_INDEX = \"<END>\", 2 # 종료 토큰\n",
        "UNK, UNK_INDEX = \"<UNK>\", 3 # 사전에 없음\n",
        "MARKER = [PAD,STD,END,UNK]\n",
        "MAX_SEQUNECE = 25"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yi8OcWcJxsMh",
        "outputId": "f7cd3df2-ce46-477a-ca9b-1f4c031a569f"
      },
      "source": [
        "# Data reading\n",
        "def load_data(path):\n",
        "    print(path)\n",
        "    df = pd.read_csv(path,header=0)\n",
        "    question, answer = list(df['Q']),list(df['A'])\n",
        "    return question, answer\n",
        "print(path)\n",
        "inputs, outputs = load_data(path+\"/songysData.csv\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gdrive/My Drive/Colab Notebooks/squad\n",
            "gdrive/My Drive/Colab Notebooks/squad/songysData.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD4uswO304Ua"
      },
      "source": [
        "# Tokenizing\n",
        "def data_tokenizer(data):\n",
        "    words = []\n",
        "    for sentence in data:\n",
        "        # 미리 컴파일한 특수문자를 제거하는 코드\n",
        "        sentence = re.sub(CHANGE_FILTER,\"\",sentence)\n",
        "        for word in sentence.split():\n",
        "            words.append(word) \n",
        "    # 공백 기준으로 단어를 나눠서 Return\n",
        "    return [word for word in words if word]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qv2fd0ud3Two"
      },
      "source": [
        "# 형태소 분리 \n",
        "def prepro_like_morphlized(data):\n",
        "    morph_analyzer= Okt()\n",
        "    results = list()\n",
        "    for seq in tqdm(data):\n",
        "        morphlized_seq = \" \".join(morph_analyzer.morphs(seq.replace(' ','')))\n",
        "        results.append(morphlized_seq)\n",
        "    return results"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCLX1fxN3VFt"
      },
      "source": [
        "# 단어 사전을 불러오는 함수\n",
        "def load_vocabulary(path, vocab_path):\n",
        "    vocabulary_list = []\n",
        "    # vocab path가 없고 -- 단어 사전파일이 없고\n",
        "    if not os.path.exists(vocab_path):\n",
        "        # Raw데이터를 불러와서 사전을 만든다.\n",
        "        # if (os.path.exists(path)):\n",
        "        df = pd.read_csv(path,encoding='utf-8')\n",
        "        question, answer = list(df['Q']),list(df['A'])\n",
        "        data = []\n",
        "        data.extend(question)\n",
        "        data.extend(answer)\n",
        "        # Tokenizing \n",
        "        words = data_tokenizer(data)\n",
        "        words = list(set(words))\n",
        "        words[:0] = MARKER # 사전에 정의한 토큰을 단어 리스트 앞에 추가\n",
        "            # print(vocab_path)\n",
        "        # print(words)\n",
        "        with open(vocab_path, 'w', encoding = 'utf-8') as vocabulary_file:\n",
        "            for word in words:\n",
        "                # print(word)\n",
        "                vocabulary_file.write(word + '\\n')\n",
        "\n",
        "    \n",
        "        \n",
        "    with open(vocab_path, 'r', encoding='utf-8') as vocabulary_file:\n",
        "        for line in vocabulary_file:\n",
        "            # print(line)\n",
        "            vocabulary_list.append(line.strip())\n",
        "    # print(vocabulary_list) \n",
        "    word2idx, idx2word = make_vocabulary(vocabulary_list)\n",
        "    \n",
        "    return word2idx, idx2word, len(word2idx)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIAG9HqT3XJF"
      },
      "source": [
        "def make_vocabulary(vocabulary_list):\n",
        "    word2idx = {word: idx for idx, word in enumerate(vocabulary_list)}\n",
        "    idx2word = {idx: word for idx, word in enumerate(vocabulary_list)}\n",
        "\n",
        "    return word2idx, idx2word"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh5-6-0J3Ytf"
      },
      "source": [
        "# 인코더와 디코더 부분 처리하기\n",
        "def enc_processing(value, dictionary):\n",
        "    sequences_input_index = []\n",
        "    sequences_length = []\n",
        "\n",
        "    for sequence in value :\n",
        "        sequence = re.sub(CHANGE_FILTER,\"\",sequence)\n",
        "        sequence_index = []\n",
        "        \n",
        "        for word in sequence.split(): # 공백 기준으로 word를 구분\n",
        "            if dictionary.get(word) is not None : # 사전에 있으면\n",
        "                sequence_index.extend([dictionary[word]]) # index 값 쓰고\n",
        "            else:\n",
        "                sequence_index.extend([dictionary[UNK]])\n",
        "        # 길이 제한\n",
        "        if len(sequence_index) > MAX_SEQUNECE:\n",
        "            sequence_index = sequence_index[:MAX_SEQUNECE]\n",
        "\n",
        "        sequences_length.append(len(sequence_index)) # 이 문장의 길이 저장\n",
        "        # Padding 추가\n",
        "        # \"안녕\"  → \"안녕,<PAD>,<PAD>,<PAD>,<PAD>\"\n",
        "        \n",
        "        sequence_index += (MAX_SEQUNECE - len(sequence_index))*[dictionary[PAD]]\n",
        "        \n",
        "        sequences_input_index.append(sequence_index)\n",
        "\n",
        "    return np.asarray(sequences_input_index), sequences_length"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JigrBthZ3aRN"
      },
      "source": [
        "# Decoder input\n",
        "\n",
        "def dec_output_processing(value, dictionary):\n",
        "    sequences_output_index = []\n",
        "    sequences_length = []\n",
        "\n",
        "    for sequence in value:\n",
        "        sequence = re.sub(CHANGE_FILTER,\"\",sequence)\n",
        "        sequence_index = []\n",
        "        # 앞부분에 시작을 알리는 토큰 넣기\n",
        "        sequence_index = [dictionary[STD]]+[dictionary[word] for word in sequence.split()]\n",
        "\n",
        "        if len(sequence_index) > MAX_SEQUNECE:\n",
        "            sequence_index = sequence_index[:MAX_SEQUNECE]\n",
        "\n",
        "        sequences_length.append(len(sequence_index))\n",
        "        sequence_index += (MAX_SEQUNECE - len(sequence_index))*[dictionary[PAD]]\n",
        "\n",
        "        sequences_output_index.append(sequence_index)\n",
        "    return np.asarray(sequences_output_index), sequences_length"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU3rRTBl3b4V"
      },
      "source": [
        "# 디코더 Target 값 전처리\n",
        "def dec_target_processing(value,dictionary):\n",
        "    sequences_target_index = []\n",
        "    for sequence in value :\n",
        "        sequence = re.sub(CHANGE_FILTER,\"\", sequence)\n",
        "        sequence_index = [dictionary[word] for word in sequence.split() ]\n",
        "        if len(sequence_index)>= MAX_SEQUNECE:\n",
        "            # 이부분이 Decoder 입력값 전처리와 다른점\n",
        "            sequence_index = sequence_index[:MAX_SEQUNECE-1] + [dictionary[END]] #마지막에 END xhzms\n",
        "        else :\n",
        "            sequence_index += [dictionary[END]]\n",
        "\n",
        "        sequence_index += (MAX_SEQUNECE - len(sequence_index))*[dictionary[PAD]]\n",
        "        sequences_target_index.append(sequence_index)\n",
        "\n",
        "    return np.asarray(sequences_target_index)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDmGDmcm3dT_",
        "outputId": "620bd54d-a59d-44f6-e083-da94c8a38e57"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    PATH = path+\"/songysData.csv\"\n",
        "    VOCAB_PATH = path+\"/vocabulary.txt\"\n",
        "    # 데이터 부르기\n",
        "    inputs, outputs = load_data(PATH)\n",
        "    # 단어 사전 부르기\n",
        "    # 토크나이저를 사용하여 처리하도록 변경하기\n",
        "    char2idx, idx2char, vocab_size = load_vocabulary(PATH,VOCAB_PATH)\n",
        "    # print(char2idx)\n",
        "\n",
        "    # encoder/decoder input /target\n",
        "    index_inputs, input_seq_len = enc_processing(inputs, char2idx)\n",
        "    index_outputs, output_seq_len = dec_output_processing(outputs, char2idx)\n",
        "    index_targets =  dec_target_processing(outputs, char2idx)\n",
        "\n",
        "    data_configs = {}\n",
        "    data_configs['char2idx'] =char2idx\n",
        "    data_configs['idx2char'] = idx2char\n",
        "    data_configs['vocab_size'] = vocab_size\n",
        "    data_configs['pad_symbol'] = PAD\n",
        "    data_configs['std_symbol'] = STD\n",
        "    data_configs['end_symbol'] = END\n",
        "    data_configs['unk_symbol'] = UNK\n",
        "\n",
        "    DATA_IN_PATH = path\n",
        "    np.save(open(DATA_IN_PATH+'/train_inputs.npy','wb'), index_inputs)\n",
        "    np.save(open(DATA_IN_PATH+'/train_outputs.npy','wb'), index_outputs)\n",
        "    np.save(open(DATA_IN_PATH+'/train_targets.npy','wb'), index_targets)\n",
        "\n",
        "    json.dump(data_configs, open(DATA_IN_PATH+'data_configs.json','w'))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gdrive/My Drive/Colab Notebooks/squad/songysData.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Nkv8ympp3gLN",
        "outputId": "1a8d7b91-d391-451b-e1aa-949267b48b35"
      },
      "source": [
        "'''전처리 결과'''\n",
        "seed = 99\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# 인코더의 입력값\n",
        "index_inputs = np.load(open(path+'/train_inputs.npy','rb'), allow_pickle=True)\n",
        "# 디코더의 입력값\n",
        "index_outputs = np.load(open(path+'/train_outputs.npy','rb'), allow_pickle=True)\n",
        "# 디코더의 타깃값\n",
        "index_targets = np.load(open(path+'/train_targets.npy','rb'), allow_pickle=True)\n",
        "# dictonary\n",
        "prepro_configs = json.load(open(path+'/data_configs.json'))\n",
        "\n",
        "'''\n",
        "인코더 Input : 최대 길이만큼 <PAD>\n",
        "디코더 Input : 시작을 알리는 <SOS>\n",
        "디코더 타겟 : 끝을 알리는 <END>\n",
        "'''"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n인코더 Input : 최대 길이만큼 <PAD>\\n디코더 Input : 시작을 알리는 <SOS>\\n디코더 타겟 : 끝을 알리는 <END>\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj03w1nS52jh"
      },
      "source": [
        "BATCH_SIZE = 2  # set을 키워보자 -> NoneType 에러가 발생한다. - 메모리이슈\n",
        "MAX_SEQUENCE = 25\n",
        "EPOCH = 5\n",
        "UNITS =1024\n",
        "EMBEDDING_DIM = 256\n",
        "VALIDATION_SPLIT = 0.1\n",
        "\n",
        "char2idx = prepro_configs['char2idx']\n",
        "idx2char = prepro_configs['idx2char']\n",
        "std_index = prepro_configs['std_symbol']\n",
        "end_index = prepro_configs['end_symbol']\n",
        "vocab_size = prepro_configs['vocab_size']"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFg3lvrU62OY"
      },
      "source": [
        "''' Encoder '''\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
        "        super(Encoder,self).__init__()\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.enc_units = enc_units\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units, \n",
        "                                         return_sequences= True,\n",
        "                                         return_state= True,\n",
        "                                         # Xavier 초기화 = Glorot 초기화 방법\n",
        "                                         # 이전 노드와 다음 노드의 개수에 의존하여 초기화 하는 방법\n",
        "                                         recurrent_initializer= 'glorot_uniform'\n",
        "                                        )\n",
        "    def call(self,x,hidden): # 입력값 X와 은닉 상태 Hidden을 받는다.\n",
        "        x = self.embedding(x)\n",
        "        output,state = self.gru(x, initial_state = hidden)\n",
        "\n",
        "        return output, state\n",
        "\n",
        "    #초기에 사용될 Hidden state를 만듦\n",
        "    def initialize_hidden_state(self, inp):\n",
        "        return tf.zeros((tf.shape(inp)[0],self.enc_units))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8nwumnI63sf"
      },
      "source": [
        "# BandanauAttention : Attention 가중치도 같이 학습 시키는 것\n",
        "class BandanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self,units): # 출력 벡터의 크기를 인자로 받음 \n",
        "        super(BandanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "    \n",
        "    def call(self, query, values): # 인코더 Hidden(query) + encoder output(values) -- 기존 Context + Hidden\n",
        "        # query와 w2를 행렬곱 할 수 있도록 shape을 바꿈\n",
        "        hidden_with_time_axis =  tf.expand_dims(query,1)\n",
        "        # W1,W2의 결과를 더하여 activation function을 취함\n",
        "        # Query와 value 에 가중치를 곱함\n",
        "        score = self.V(tf.nn.tanh(\n",
        "                                self.W1(values)+self.W2(hidden_with_time_axis)\n",
        "                ))\n",
        "        attention_weights = tf.nn.softmax(score,axis=1)\n",
        "        \n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis =1) # 행단위로 Sum 하는 것\n",
        "        \n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39oL48XD64zC"
      },
      "source": [
        "\n",
        "''' Decoder '''\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self,vocab_size, embedding_dim, dec_units, batch_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.dec_units =  dec_units\n",
        "        \n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                       return_sequences = True,\n",
        "                                        return_state = True,\n",
        "                                        recurrent_initializer = 'glorot_uniform'\n",
        "                                       )\n",
        "        self.fc = tf.keras.layers.Dense(self.vocab_size)\n",
        "        self.attention = BandanauAttention(self.dec_units)\n",
        "        \n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # 디코더의 입력값 x, 인코더의 은닉 상태값 hidden, 인코더의 결과값 enc_output\n",
        "        # 인코딩이 Query, attention이 key, 인코더 결과가 values?\n",
        "        \n",
        "        context_vector,attention_weights = self.attention(hidden, enc_output) \n",
        "        x = self.embedding(x)\n",
        "\n",
        "        x = tf.concat([tf.expand_dims(context_vector,1),x], axis =-1)  #axis -1\n",
        "        # concat 한 결과를 LSTM 하는 것\n",
        "        output,state = self.gru(x)\n",
        "#         print(output.shape)\n",
        "#         output = tf.concat([tf.expand_dims(context_vector,1),output], axis =2) \n",
        "#         print(output.shape)\n",
        "        output = tf.reshape(output, (-1,output.shape[2]))\n",
        "#         print(output.shape)\n",
        "        x = self.fc(output)\n",
        "#         print(1234, x)\n",
        "        \n",
        "        return x, state, attention_weights\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91lYUBcE67UX"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "#크로스 엔트로피 손실값 측정\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction= 'none')\n",
        "#정확도 측정 객체\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name = 'accuracy')\n",
        "\n",
        "def loss(real, pred):# real 값 중에서 0 인 <PAD> 값 제거하기 위한 함수    \n",
        "    mask = tf.math.logical_not(tf.math.equal(real,0)) # True 1 , <PAD> 제외한 나머지는 0 \n",
        "    loss_ = loss_object(real,pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask # 요소간의 곱을 하면 <PAD>는 loss 계산에서 제외됨. True만 남고 나머지는 다 0으로 바뀌네\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "def accuracy(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real,0))\n",
        "    mask = tf.expand_dims(tf.cast(mask, dtype = pred.dtype), axis = -1)\n",
        "    pred *= mask\n",
        "    acc = train_accuracy(real, pred)\n",
        "    \n",
        "    return tf.reduce_mean(acc)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZJPSaR669I2"
      },
      "source": [
        "''' Main Class : encoding+decoding'''\n",
        "class seq2seq(tf.keras.Model):\n",
        "    def __init__(self,vocab_size, embedding_dim, enc_units, dec_units, batch_size, end_token_idx = 2):\n",
        "        super(seq2seq, self).__init__()\n",
        "        self.end_token_idx = end_token_idx\n",
        "        self.encoder = Encoder(vocab_size, embedding_dim, enc_units, batch_size)\n",
        "        self.decoder = Decoder(vocab_size, embedding_dim, dec_units, batch_size)\n",
        "        \n",
        "    def call(self,x): # x는 인코더, 디코더 입력값을 포함 함\n",
        "#         print(x[1])\n",
        "        inp, tar = x\n",
        "#         print(inp, tar, x)\n",
        "        \n",
        "        # Encoder의 Hidden vector를 초기화 하여 encoding\n",
        "        enc_hidden = self.encoder.initialize_hidden_state(inp)\n",
        "        enc_output, enc_hidden = self.encoder(inp, enc_hidden)\n",
        "        \n",
        "        dec_hidden = enc_hidden\n",
        "        \n",
        "        # 반복적으로 state 별로 attention 결과를 받아와서 Decoding\n",
        "        predict_tokens  = list()\n",
        "        for t in range(0, tar.shape[1]):\n",
        "#             print(t, tar.shape)\n",
        "            dec_input = tf.dtypes.cast(tf.expand_dims(tar[:,t],1),tf.float32) #특정 state 디코더 입력값\n",
        "            \n",
        "            predictions, dec_hidden, _ = self.decoder(dec_input, dec_hidden, enc_output)\n",
        "#             print(predictions)\n",
        "            predict_tokens.append(tf.dtypes.cast(predictions, tf.float32))\n",
        "#             print(predict_tokens)\n",
        "#         print(predict_tokens)\n",
        "        result = tf.stack(predict_tokens, axis = 1)\n",
        "#         print(1111)\n",
        "#         print(np.array(result))\n",
        "#         print(222)\n",
        "        return result\n",
        "        \n",
        "    def inference(self, x): #모델의 결과값을 확인하기 위함, Test 목적\n",
        "        inp = x\n",
        "#         print(111)\n",
        "        enc_hidden = self.encoder.initialize_hidden_state(inp)\n",
        "        enc_output,enc_hidden = self.encoder(inp,enc_hidden)\n",
        "        \n",
        "        dec_hidden = enc_hidden\n",
        "        \n",
        "        dec_input = tf.expand_dims([char2idx[std_index]],1)  #end \n",
        "        \n",
        "        predict_tokens = list()\n",
        "        for t in range(0, MAX_SEQUENCE):\n",
        "            predictions,dec_hidden, _ = self.decoder(dec_input, dec_hidden, enc_output)\n",
        "            predict_token = tf.argmax(predictions[0])\n",
        "            \n",
        "            if predict_token == self.end_token_idx : # 끝을 만나면 종료\n",
        "                break\n",
        "            predict_tokens.append(predict_token)\n",
        "            dec_input = tf.dtypes.cast(tf.expand_dims([predict_token],0),tf.float32)\n",
        "        \n",
        "        return tf.stack(predict_tokens, axis =0).numpy()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHVJ0e4U6_g4"
      },
      "source": [
        "''' Model 생성'''\n",
        "model = seq2seq(vocab_size, EMBEDDING_DIM, UNITS, UNITS,BATCH_SIZE, char2idx[end_index])\n",
        "model.compile(loss = loss, optimizer= tf.keras.optimizers.Adam(1e-3), metrics =  [accuracy])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "LrIyB70_7Bz2",
        "outputId": "390a35d3-f851-4204-9e4f-d8e62a377cbe"
      },
      "source": [
        "if not(os.path.isdir(path)):\n",
        "    os.makedirs(os.path.join(path))\n",
        "\n",
        "model.load_weights(os.path.join('gdrive/My Drive/Colab Notebooks/squad/weights.h5'))    \n",
        "chk_path = path + '/weights.h5'\n",
        "\n",
        "callback = ModelCheckpoint( chk_path, monitor = 'val_accuracy', verbose =1, save_best_only= True,\n",
        "                            save_weights_only =True)\n",
        "earlystop = EarlyStopping(monitor ='val_accuracy', min_delta = 0.001, patience =10)\n",
        "\n",
        "history = model.fit([index_inputs, index_outputs], index_targets,\n",
        "                   batch_size =BATCH_SIZE,\n",
        "                   epochs = 5,\n",
        "                   validation_split= 0.2, # set이 너무 작아서 valloss 계산이 안되는 거일수도 있다.\n",
        "                   callbacks = [earlystop, callback])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "   6/4729 [..............................] - ETA: 8:54:40 - loss: 0.2235 - accuracy: 0.9310"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-0ab6cafb5ef1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                    \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                    \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# set이 너무 작아서 valloss 계산이 안되는 거일수도 있다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                    callbacks = [earlystop, callback])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPMs8p5PA_9s"
      },
      "source": [
        "SAVE_FILE_NM = \"weights.h5\"\n",
        "model.load_weights(os.path.join('gdrive/My Drive/Colab Notebooks/squad/weights.h5'))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "p-BQKfzv7DgF",
        "outputId": "4b1d2f5b-82a3-4f2d-f98f-f98a4480561e"
      },
      "source": [
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string],'')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string,'val_'+string])\n",
        "    plt.show()\n",
        "plot_graphs(history,'accuracy')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-64387f203aed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'val_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mplot_graphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoAw9Oe4_841"
      },
      "source": [
        "plot_graphs(history,'loss')\n",
        "# 뭔가 잘못 됐다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAT8JbwaBHhA",
        "outputId": "cd8d3883-ce5d-4ebb-d665-d112085f130d"
      },
      "source": [
        "query = \"나는 어때\"\n",
        "\n",
        "test_index_inputs , _ = enc_processing([query],char2idx)\n",
        "predict_tokens =  model.inference(test_index_inputs)\n",
        "print(' '.join([idx2char['%s'%t] for  t in predict_tokens]))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "오늘 단단히 많이 생각했나봐요\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttvGgS5wDMbQ",
        "outputId": "3b2670c6-3c77-427b-ab2c-712453250930"
      },
      "source": [
        "query = \"남자친구 승진 선물로 뭐가 좋을까\"\n",
        "\n",
        "test_index_inputs , _ = enc_processing([query],char2idx)\n",
        "predict_tokens =  model.inference(test_index_inputs)\n",
        "print(' '.join([idx2char['%s'%t] for  t in predict_tokens]))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "그렇게 해보세요\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sw0vtfNODN8C",
        "outputId": "184c16c2-a196-4256-b9eb-5b7531354e51"
      },
      "source": [
        "query = \"뭐야?\"\n",
        "\n",
        "test_index_inputs , _ = enc_processing([query],char2idx)\n",
        "predict_tokens =  model.inference(test_index_inputs)\n",
        "print(' '.join([idx2char['%s'%t] for  t in predict_tokens]))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "많이 사랑했나봐요\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTXMWWlHDQpt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}