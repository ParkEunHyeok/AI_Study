{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2_Chatbot_Pruning_Testing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN0LT1p0Uma59B+4aOhRweP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParkEunHyeok/AI_Study/blob/main/NLP/GPT2_Chatbot_Pruning_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toJfeqecrldL",
        "outputId": "123228da-7222-4cf9-c575-8bae989ea641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jul 10 06:05:50 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "We7023A5wNz6",
        "outputId": "0b42dd3d-6231-47ab-81e4-d565acb103bd"
      },
      "source": [
        "!pip install tqdm\n",
        "!pip install kobert-transformers==0.4.1\n",
        "!pip install kogpt2-transformers==0.3.0\n",
        "!pip install transformers==3.0.2\n",
        "!pip install torch\n",
        "!pip install tokenizers==0.8.1rc1\n",
        "!pip install kss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kobert-transformers==0.4.1\n",
            "  Downloading kobert_transformers-0.4.1-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from kobert-transformers==0.4.1) (1.11.0+cu113)\n",
            "Collecting transformers>=2.9.1\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1.0->kobert-transformers==0.4.1) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.9.1->kobert-transformers==0.4.1) (1.21.6)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 9.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.9.1->kobert-transformers==0.4.1) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 52.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=2.9.1->kobert-transformers==0.4.1) (4.11.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.9.1->kobert-transformers==0.4.1) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=2.9.1->kobert-transformers==0.4.1) (3.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=2.9.1->kobert-transformers==0.4.1) (2.23.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 89.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.9.1->kobert-transformers==0.4.1) (4.64.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=2.9.1->kobert-transformers==0.4.1) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=2.9.1->kobert-transformers==0.4.1) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.9.1->kobert-transformers==0.4.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.9.1->kobert-transformers==0.4.1) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.9.1->kobert-transformers==0.4.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.9.1->kobert-transformers==0.4.1) (1.24.3)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers, kobert-transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 kobert-transformers-0.4.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kogpt2-transformers==0.3.0\n",
            "  Downloading kogpt2_transformers-0.3.0-py3-none-any.whl (4.6 kB)\n",
            "Requirement already satisfied: tokenizers>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from kogpt2-transformers==0.3.0) (0.12.1)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from kogpt2-transformers==0.3.0) (4.20.1)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from kogpt2-transformers==0.3.0) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1.0->kogpt2-transformers==0.3.0) (4.1.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->kogpt2-transformers==0.3.0) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->kogpt2-transformers==0.3.0) (4.11.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->kogpt2-transformers==0.3.0) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->kogpt2-transformers==0.3.0) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->kogpt2-transformers==0.3.0) (3.7.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->kogpt2-transformers==0.3.0) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->kogpt2-transformers==0.3.0) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->kogpt2-transformers==0.3.0) (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0->kogpt2-transformers==0.3.0) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=3.0.0->kogpt2-transformers==0.3.0) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=3.0.0->kogpt2-transformers==0.3.0) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.0.0->kogpt2-transformers==0.3.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.0.0->kogpt2-transformers==0.3.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.0.0->kogpt2-transformers==0.3.0) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.0.0->kogpt2-transformers==0.3.0) (1.24.3)\n",
            "Installing collected packages: kogpt2-transformers\n",
            "Successfully installed kogpt2-transformers-0.3.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==3.0.2\n",
            "  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n",
            "\u001b[K     |████████████████████████████████| 769 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "  Downloading tokenizers-0.8.1rc1-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 69.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 76.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2022.6.2)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 76.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (3.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2022.6.15)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=05a12f0d2981d467f948994aed401f978a31bff9e36fe5c289acad5c3b6f23f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.12.1\n",
            "    Uninstalling tokenizers-0.12.1:\n",
            "      Successfully uninstalled tokenizers-0.12.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.20.1\n",
            "    Uninstalling transformers-4.20.1:\n",
            "      Successfully uninstalled transformers-4.20.1\n",
            "Successfully installed sacremoses-0.0.53 sentencepiece-0.1.96 tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tokenizers==0.8.1rc1 in /usr/local/lib/python3.7/dist-packages (0.8.1rc1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kss\n",
            "  Downloading kss-3.4.2.tar.gz (42.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.4 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 65.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from kss) (2022.6.2)\n",
            "Requirement already satisfied: more_itertools in /usr/local/lib/python3.7/dist-packages (from kss) (8.13.0)\n",
            "Building wheels for collected packages: kss, emoji\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-3.4.2-py3-none-any.whl size=42448069 sha256=9985e47e05f67463eec910fd7ca7ba422bc364ac58a02f3c9bd28dfc46a59170\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/22/aa/6399b60516a067ec97fa6599fb2d472aeb25e3f9ee6dae3224\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=b154cf70ed368a18e1c6446cc9fbd026fead016e88cb5fdde9a3b2a3c49c8453\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built kss emoji\n",
            "Installing collected packages: emoji, kss\n",
            "Successfully installed emoji-1.7.0 kss-3.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BS9l6LkNwlnj",
        "outputId": "99718e9d-b167-4a53-b3c5-c81eb244ef6a"
      },
      "source": [
        "# 구글 드라이브 연결\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "path = \"gdrive/My Drive/Colab Notebooks/HelloNewWorld/gpt\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C27S2uXv70f"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from kogpt2_transformers import get_kogpt2_tokenizer, get_kogpt2_model\n",
        "import torch.nn.utils.prune as prune"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Cbhh1nLw52U"
      },
      "source": [
        "class DialogKoGPT2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DialogKoGPT2, self).__init__()\n",
        "    self.kogpt2 = get_kogpt2_model()\n",
        "\n",
        "  def generate(self,\n",
        "               input_ids,\n",
        "               do_sample=True,\n",
        "               max_length= 60,\n",
        "               top_p=0.92,\n",
        "               top_k=50,\n",
        "               temperature= 0.6,\n",
        "               no_repeat_ngram_size = None,\n",
        "               num_return_sequences=2,\n",
        "               early_stopping=False,\n",
        "               ):\n",
        "    return self.kogpt2.generate(input_ids,\n",
        "               do_sample=do_sample,\n",
        "               max_length=max_length,\n",
        "               top_p = top_p,\n",
        "               top_k=top_k,\n",
        "               temperature=temperature,\n",
        "               no_repeat_ngram_size= no_repeat_ngram_size,\n",
        "               num_return_sequences=num_return_sequences,\n",
        "               early_stopping = early_stopping,\n",
        "              )\n",
        "\n",
        "  def forward(self, input, labels = None):\n",
        "    if labels is not None:\n",
        "      outputs = self.kogpt2(input, labels=labels)\n",
        "    else:\n",
        "      outputs = self.kogpt2(input)\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global Pruning 적용\n",
        "ctx = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = torch.device(ctx)\n",
        "\n",
        "model = DialogKoGPT2()\n",
        "model.to(device)\n",
        "\n",
        "parameters_to_prune = ()\n",
        "for i in range(12):\n",
        "    parameters_to_prune += (\n",
        "        (model.kogpt2.transformer.h[i].attn.c_attn, 'weight'),\n",
        "        (model.kogpt2.transformer.h[i].attn.c_proj, 'weight'),\n",
        "    )\n",
        "\n",
        "prune.global_unstructured(\n",
        "    parameters_to_prune,\n",
        "    pruning_method=prune.L1Unstructured,\n",
        "    amount=0.2,\n",
        ")"
      ],
      "metadata": {
        "id": "mSZM5IVbs6l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global Pruning을 통해 전체 Sparsity를 20%로 만들 수 있음을 알 수 있음.\n",
        "\n",
        "for i in range(12):\n",
        "    print(\n",
        "        \"Sparsity in Layer {}-th c_attn weight: {:.2f}%\".format(\n",
        "            i+1,\n",
        "            100. * float(torch.sum(model.kogpt2.transformer.h[i].attn.c_attn.weight == 0))\n",
        "            / float(model.kogpt2.transformer.h[i].attn.c_attn.weight.nelement())\n",
        "        )\n",
        "    )\n",
        "    print(\n",
        "        \"Sparsity in Layer {}-th c_proj weight: {:.2f}%\".format(\n",
        "            i+1,\n",
        "            100. * float(torch.sum(model.kogpt2.transformer.h[i].attn.c_proj.weight == 0))\n",
        "            / float(model.kogpt2.transformer.h[i].attn.c_proj.weight.nelement())\n",
        "        )\n",
        "    )\n",
        "    print()\n",
        "\n",
        "    \n",
        "numerator, denominator = 0, 0\n",
        "for i in range(12):\n",
        "    numerator += torch.sum(model.kogpt2.transformer.h[i].attn.c_attn.weight == 0)\n",
        "    numerator += torch.sum(model.kogpt2.transformer.h[i].attn.c_proj.weight == 0)\n",
        "\n",
        "    denominator += model.kogpt2.transformer.h[i].attn.c_attn.weight.nelement()\n",
        "    denominator += model.kogpt2.transformer.h[i].attn.c_proj.weight.nelement()\n",
        "    \n",
        "print(\"Global sparsity: {:.2f}%\".format(100. * float(numerator) / float(denominator)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhXIMXdDtAM_",
        "outputId": "42fcb0ab-7b39-4510-9f9e-d1cbdbe6cff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sparsity in Layer 1-th c_attn weight: 28.47%\n",
            "Sparsity in Layer 1-th c_proj weight: 65.74%\n",
            "\n",
            "Sparsity in Layer 2-th c_attn weight: 20.61%\n",
            "Sparsity in Layer 2-th c_proj weight: 24.94%\n",
            "\n",
            "Sparsity in Layer 3-th c_attn weight: 18.73%\n",
            "Sparsity in Layer 3-th c_proj weight: 27.17%\n",
            "\n",
            "Sparsity in Layer 4-th c_attn weight: 17.68%\n",
            "Sparsity in Layer 4-th c_proj weight: 23.86%\n",
            "\n",
            "Sparsity in Layer 5-th c_attn weight: 17.33%\n",
            "Sparsity in Layer 5-th c_proj weight: 21.64%\n",
            "\n",
            "Sparsity in Layer 6-th c_attn weight: 17.04%\n",
            "Sparsity in Layer 6-th c_proj weight: 19.34%\n",
            "\n",
            "Sparsity in Layer 7-th c_attn weight: 17.24%\n",
            "Sparsity in Layer 7-th c_proj weight: 18.92%\n",
            "\n",
            "Sparsity in Layer 8-th c_attn weight: 17.60%\n",
            "Sparsity in Layer 8-th c_proj weight: 19.29%\n",
            "\n",
            "Sparsity in Layer 9-th c_attn weight: 16.98%\n",
            "Sparsity in Layer 9-th c_proj weight: 19.14%\n",
            "\n",
            "Sparsity in Layer 10-th c_attn weight: 17.09%\n",
            "Sparsity in Layer 10-th c_proj weight: 17.16%\n",
            "\n",
            "Sparsity in Layer 11-th c_attn weight: 17.65%\n",
            "Sparsity in Layer 11-th c_proj weight: 15.29%\n",
            "\n",
            "Sparsity in Layer 12-th c_attn weight: 17.77%\n",
            "Sparsity in Layer 12-th c_proj weight: 14.94%\n",
            "\n",
            "Global sparsity: 20.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRjjr-vTwVz0"
      },
      "source": [
        "root_path=path\n",
        "data_path = f\"{root_path}/data/wellness_dialog_for_autoregressive.txt\"\n",
        "checkpoint_path =f\"{root_path}/checkpoint\"\n",
        "# save_ckpt_path = f\"{checkpoint_path}/kogpt2-wellnesee-auto-regressive-20210922-add-chatbotdata.pth\"\n",
        "save_ckpt_path = f\"{checkpoint_path}/kogpt2-20220710-add-chatbotdata.pth\"\n",
        "\n",
        "ctx = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = torch.device(ctx)\n",
        "\n",
        "# 저장한 Checkpoint 불러오기\n",
        "checkpoint = torch.load(save_ckpt_path, map_location=device)\n",
        "\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "tokenizer = get_kogpt2_tokenizer()\n",
        "\n",
        "count = 0\n",
        "output_size = 200 # 출력하고자 하는 토큰 갯수"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XjbY2lwZxF5l",
        "outputId": "4e35e84b-30a9-4149-fd91-9bf9e59b5239"
      },
      "source": [
        "while 1:\n",
        "#for i in range(5):\n",
        "  sent = input('Question: ')  # '요즘 기분이 우울한 느낌이에요'\n",
        "  tokenized_indexs = tokenizer.encode(sent)\n",
        "\n",
        "  input_ids = torch.tensor([tokenizer.bos_token_id,]  + tokenized_indexs +[tokenizer.eos_token_id]).unsqueeze(0).to(device)\n",
        "  # set top_k to 50\n",
        "  sample_output = model.generate(\n",
        "      input_ids=input_ids\n",
        "      ).to(device)\n",
        "\n",
        "\n",
        "  print(\"Answer: \" + tokenizer.decode(sample_output[0].tolist()[len(tokenized_indexs)+1:],skip_special_tokens=True))\n",
        "  print(100 * '-')\n",
        "  # print(sample_output.shape)\n",
        "\n",
        "# for s in kss.split_sentences(sent):\n",
        "#     print(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: 요즘 기분이 우울한 느낌이에요.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 우울함은 저절로 없어지기도 하지만 그렇지 않을 때도 있어요. 그럴 때는 전문가에게 도움을 요청하는 것도 좋은 방법이에요. 도움을 받는 것도 좋지만 이렇게 우울한 기분이 오래 지속되면 정말 힘들죠. 우울한 기분이 오래 지속되면 정말 힘들죠\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question: 요새 힘들어요\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 힘들면 울어도 돼요. 티슈를 준비해드릴까요?용기내서 이야기해보세요. 듣고 있으니 조금만 더 기다려주세요. 용기를 내보세요. 말씀해주세요. 더 들려주세요. 더 들려\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question: 너가 좋아하는건 뭐니?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 그건 중요하지 않아요. 관심 없는 게 더 좋을지도 몰라요. 관심 없다면 실망하지 마세요. 관심 없다면 실망하지 마세요. 찬성이 좋을지도 몰라요. 찬성이는 하나밖에 없는 소중한\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question: 체력이 떨어져서 문제야\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 운동을 해보세요. 근력이란 놀라울 정도로 탄탄하죠. 천천히 심호흡을 해보세요. 천천히 심호흡을 해보세요. 천천히 심호흡을 해보세요. 천천히 심호흡을 해보세요.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question: 요즘 잠이 안와요\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 생각이 많은가봅니다. 생각을 정리할 시간이 필요하시군요.생각 정리를 해보세요. 정리할 시간이 필요할지도 몰라요.정리해보세요.생각 정리를 하다 보면 조금씩 나아질 거예요.마음이\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question: 너무 졸려요\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 잠을 깨요! 기운 내요!! 그럼 새벽이 올까요? 수유리 갈맷길 참 예쁘게 타요! 감기 조심하세요! 그럼 곧 퇴원할 수 있을 거예요. 그럼 저는 외출할 준비를 하고 있어요.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question: 스트레스가 있긴 했는데, 잠을 좀 설치는 거 말고는 다른 건 없었어.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 스트레스가 많으면 정말 힘들죠. 해소할 수 있는 방법을 찾아 보는 건 어떨까요? 관대해진다는 느낌은 좋지 않을 거예요. 스스로에게 보내는 응원이 가장\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question: 공부하기 싫어\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 지금도 늦지 않았어요.조금 더 마음을 달리 가져보세요.지금도 늦지 않았어요.지금도 늦지 않았어요.마음이 조금 차분해 질 거예요.지금도 늦지 않아요.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question: 위로해줘\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 제가 위로해 드릴게요. 그럼 안녕히 주무세요. 정말 보고 싶으셨나봐요. 저한테 계속 말씀해주세요. 그럼 수줍게 웃어주세요. 그러면 더 많이 힘들 거예요. 그러면\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \"\"\"\n\u001b[0;32m--> 727\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-8675686c2864>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#for i in range(5):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Question: '\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# '요즘 기분이 우울한 느낌이에요'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mtokenized_indexs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh2v1mnzP-Mb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}