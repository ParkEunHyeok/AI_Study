{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_Seq2Seq_translation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNPRyQBq221L6GPlsX940JC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParkEunHyeok/AI_Study/blob/main/Pytorch/Pytorch_Seq2Seq_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKfjPcCiSZzp"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "import spacy\n",
        "import spacy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8c9N3-6FGap",
        "outputId": "f237d103-6240-48c6-93e1-6513ed1a04fb"
      },
      "source": [
        "# 구글 드라이브 연결\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "path = \"gdrive/My Drive/Colab Notebooks/seq2seq/pytorch_seq2seq_translation/\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4KMaxpRcFhN",
        "outputId": "285d234d-b57a-4ab8-8e38-42acaa8804f5"
      },
      "source": [
        "# 문장을 토큰화 하는 모듈 설치\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.6.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Collecting de_core_news_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.9 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.62.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (4.6.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Building wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-py3-none-any.whl size=14907055 sha256=2ec15b48580490729cf6e51e76b10903941d39e39d4c4d3fca8eef453b824340\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-uq_8wjm1/wheels/00/66/69/cb6c921610087d2cab339062345098e30a5ceb665360e7b32a\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQxCD6A1cTCF"
      },
      "source": [
        "import de_core_news_sm\n",
        "import en_core_web_sm\n",
        "\n",
        "# 문장 토큰화 모델 load\n",
        "spacy_en = en_core_web_sm.load()\n",
        "spacy_de = de_core_news_sm.load()\n",
        "\n",
        "# 토큰화 함수,\n",
        "# 입력문장의 단어를 뒤집어 성능을 향상시킴\n",
        "def tokenize_de(text):\n",
        "  return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
        "\n",
        "def tokenize_en(text):\n",
        "  return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "# Field를 사용하여 구체적인 전처리 내용 명시\n",
        "# source : 독일어\n",
        "SRC = Field(tokenize=tokenize_de,\n",
        "            init_token=\"<SOS>\",\n",
        "            eos_token=\"<EOS>\",\n",
        "            lower=True)\n",
        "\n",
        "# target : 영어\n",
        "TRG = Field(tokenize=tokenize_en,\n",
        "            init_token=\"<SOS>\",\n",
        "            eos_token=\"<EOS>\",\n",
        "            lower=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnXbaMFBdlwG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "114e40cf-f2cb-4040-874c-1feb84b5b745"
      },
      "source": [
        "# Multi30k 데이터셋 불러오기\n",
        "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),\n",
        "                                                    fields = (SRC, TRG))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading training.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:02<00:00, 537kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading validation.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 92.3kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 88.2kB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDtMPuDEefFu",
        "outputId": "db10e91d-3c6e-4c5b-a065-9f2cd917420a"
      },
      "source": [
        "# 데이터의 크기 명시\n",
        "print(f'Number of training examples: {len(train_data.examples)}')\n",
        "print(f'Number of validation examples: {len(valid_data.examples)}')\n",
        "print(f'Number of testing examples: {len(test_data.examples)}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 29000\n",
            "Number of validation examples: 1014\n",
            "Number of testing examples: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWQ0LY0oewCV"
      },
      "source": [
        "# min_freq = 2는 두 번 이상 등장한 토큰을 출력\n",
        "# token이 한 번만 등장했다면, <unk>로 대체\n",
        "SRC.build_vocab(train_data, min_freq=2)\n",
        "TRG.build_vocab(train_data, min_freq=2)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tohj2Uf3fCdT"
      },
      "source": [
        "# iterator 생성\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "# 하나의 배치에 포함되는 단어의 개수가 유사하도록 만듦\n",
        "# batch size = 128\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), batch_size=batch_size, device=device\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuRVLcokfFGC"
      },
      "source": [
        "# 주어진 Sequence를 입력받아 context vector로 인코딩\n",
        "# LSTM모델은 hidden state와 cell state를 반환\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.hid_dim = hid_dim\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    # 입력값을 emd_dim 벡터로 인코딩\n",
        "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "\n",
        "    # 임베딩을 입력받아 hid_dim 크기의 hidden state, cell 출력\n",
        "    self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, src):\n",
        "    embedded = self.dropout(self.embedding(src))\n",
        "\n",
        "    outputs, (hidden, cell) = self.rnn(embedded)\n",
        "\n",
        "    return hidden, cell"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ0jQEnZgc96"
      },
      "source": [
        "# encode된 context vector를 입력받아 decode해 단어를 예측\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.output_dim = output_dim\n",
        "    self.hid_dim = hid_dim\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    # context vector를 입력받아 emb_dim 출력\n",
        "    self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "\n",
        "    # embedding을 입력받아 hid_dim 크기의 hidden state, cell 출력\n",
        "    self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
        "\n",
        "    self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, input, hidden, cell):\n",
        "    input = input.unsqueeze(0)\n",
        "\n",
        "    embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "    output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "\n",
        "    # 예측값의 fc score\n",
        "    prediction = self.fc_out(output.squeeze(0))\n",
        "\n",
        "    return prediction, hidden, cell"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6B0ulwviHgb"
      },
      "source": [
        "# 앞서 정의한 encoder, decoder를 가지고 있는 하나의 아키텍쳐\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, devide):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.device = device\n",
        "\n",
        "    # encoder와 decoder의 hid_dim이 일치하지 않는 경우 에러메세지\n",
        "    assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "        'Hidden dimensions of encoder decoder must be equal'\n",
        "    # encoder와 decoder의 n_layers가 일치하지 않는 경우 에러메세지\n",
        "    assert encoder.n_layers == decoder.n_layers, \\\n",
        "        'Encoder and decoder must have equal number of layers'\n",
        "\n",
        "  def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "    batch_size = trg.shape[1]\n",
        "    trg_len = trg.shape[0]  # 타겟 토큰 길이 얻기\n",
        "    trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "    # decoder의 output을 저장하기 위한 텐서\n",
        "    outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "    # initial hidden state\n",
        "    hidden, cell = self.encoder(src)\n",
        "\n",
        "    # 첫 번째 입력값 <SOS> 토큰\n",
        "    input = trg[0, :]\n",
        "\n",
        "    for t in range(1, trg_len): # <EOS> 제외하고 타겟 길이-1 만큼 반복\n",
        "      output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "\n",
        "      # prediction 저장\n",
        "      outputs[t] = output\n",
        "\n",
        "      # 교사 강요를 사용할지 말지 결정\n",
        "      teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "      # 가장 높은 확률을 갖은 갚\n",
        "      top1 = output.argmax(1)\n",
        "\n",
        "      # 교사 강요를 할 경우 다음 LSTM에 타겟 토큰 입력\n",
        "      input = trg[t] if teacher_force else top1\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8YCLlnwjnzp"
      },
      "source": [
        "# 하이퍼 파라미터 지정\n",
        "input_dim = len(SRC.vocab)\n",
        "output_dim = len(TRG.vocab)\n",
        "enc_emb_dim = 256 # 임베딩 차원\n",
        "dec_emb_dim = 256\n",
        "hid_dim = 512 # hidden state 차원\n",
        "n_layers = 2\n",
        "enc_dropout = 0.5\n",
        "dec_dropout = 0.5"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0hRVRTdjz8M"
      },
      "source": [
        "# 모델 생성\n",
        "enc = Encoder(input_dim, enc_emb_dim, hid_dim, n_layers, enc_dropout)\n",
        "dec = Decoder(output_dim, dec_emb_dim, hid_dim, n_layers, dec_dropout)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mni9n_yVj1Xb",
        "outputId": "904c6a03-fefd-41e5-8060-637cc744e2b8"
      },
      "source": [
        "# 가중치 초기화\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(7855, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(5923, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (fc_out): Linear(in_features=512, out_features=5923, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xe5W3ugij5QK",
        "outputId": "cb9df504-1553-418e-c917-06ea76218b3a"
      },
      "source": [
        "# 모델의 학습가능한 파라미터 수 측정\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainableparameters')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 13,922,083 trainableparameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VddhhAOLj8wC"
      },
      "source": [
        "# optimizer\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# loss function\n",
        "# pad에 해당하는 index는 무시\n",
        "trg_pad_idx = TRG.vocab.stoi[TRG.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtlfkeODHoKk",
        "outputId": "d5efc0fe-954d-439e-dcc5-f935c97e6016"
      },
      "source": [
        "import os.path\n",
        "if os.path.isfile(path+'tut1-model.pt'):\n",
        "  print(\"loaded model.\")\n",
        "  model = torch.load(path+'tut1-model.pt')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc65MBiZj_di"
      },
      "source": [
        "# 학습을 위한 함수\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src,trg) # [trg len, batch size, output dim]\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].view(-1, output_dim) # loss 계산을 위해 1d로 변경\n",
        "        trg = trg[1:].view(-1) # loss 계산을 위해 1d로 변경\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "\n",
        "        # 기울기 clip\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nkwjI1BkBTq"
      },
      "source": [
        "# evaluation function\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            # output: [trg len, batch size, output dim]\n",
        "            output = model(src, trg, 0) # teacher forcing off\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[1:].view(-1, output_dim) # [(trg len -1) * batch size, output dim]\n",
        "            trg = trg[1:].view(-1) # [(trg len -1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKzPY6q6kCvz"
      },
      "source": [
        "# function to count training time\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0R3mxpwkEKR",
        "outputId": "5c25026c-1998-4810-f97b-5542e3fb5f01"
      },
      "source": [
        "# 학습 시작\n",
        "num_epochs = 10\n",
        "clip = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "   \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, clip)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        # torch.save(model.state_dict(), path+'tut1-model.pt')\n",
        "        torch.save(model, path+'tut1-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 1m 13s\n",
            "\tTrain Loss: 3.153 | Train PPL:  23.415\n",
            "\t Val. Loss: 3.927 |  Val. PPL:  50.754\n",
            "Epoch: 02 | Time: 1m 13s\n",
            "\tTrain Loss: 3.146 | Train PPL:  23.232\n",
            "\t Val. Loss: 3.927 |  Val. PPL:  50.754\n",
            "Epoch: 03 | Time: 1m 13s\n",
            "\tTrain Loss: 3.162 | Train PPL:  23.608\n",
            "\t Val. Loss: 3.927 |  Val. PPL:  50.754\n",
            "Epoch: 04 | Time: 1m 13s\n",
            "\tTrain Loss: 3.156 | Train PPL:  23.481\n",
            "\t Val. Loss: 3.927 |  Val. PPL:  50.754\n",
            "Epoch: 05 | Time: 1m 13s\n",
            "\tTrain Loss: 3.140 | Train PPL:  23.112\n",
            "\t Val. Loss: 3.927 |  Val. PPL:  50.754\n",
            "Epoch: 06 | Time: 1m 12s\n",
            "\tTrain Loss: 3.161 | Train PPL:  23.584\n",
            "\t Val. Loss: 3.927 |  Val. PPL:  50.754\n",
            "Epoch: 07 | Time: 1m 13s\n",
            "\tTrain Loss: 3.137 | Train PPL:  23.039\n",
            "\t Val. Loss: 3.927 |  Val. PPL:  50.754\n",
            "Epoch: 08 | Time: 1m 12s\n",
            "\tTrain Loss: 3.156 | Train PPL:  23.477\n",
            "\t Val. Loss: 3.927 |  Val. PPL:  50.754\n",
            "Epoch: 09 | Time: 1m 13s\n",
            "\tTrain Loss: 3.156 | Train PPL:  23.483\n",
            "\t Val. Loss: 3.927 |  Val. PPL:  50.754\n",
            "Epoch: 10 | Time: 1m 13s\n",
            "\tTrain Loss: 3.140 | Train PPL:  23.114\n",
            "\t Val. Loss: 3.927 |  Val. PPL:  50.754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fFdnTkWkFfD",
        "outputId": "9b99729a-30bb-4a19-9a8f-a5fe01087a2d"
      },
      "source": [
        "# best val loss일 때의 가중치를 불러옵니다.\n",
        "torch.load(path+'tut1-model.pt')\n",
        "\n",
        "# test loss를 측정합니다.\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Test Loss: 3.973 | Test PPL:  53.146 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkVJpetVp8h2"
      },
      "source": [
        "# 번역(translation) 함수\n",
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len=50):\n",
        "    model.eval() # 평가 모드\n",
        "\n",
        "    if isinstance(sentence, str):\n",
        "        nlp = spacy.load('de')\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    # 처음에 <sos> 토큰, 마지막에 <eos> 토큰 붙이기\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "    print(f\"전체 소스 토큰: {tokens}\")\n",
        "\n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "    print(f\"소스 문장 인덱스: {src_indexes}\")\n",
        "\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "\n",
        "    # 인코더(endocer)에 소스 문장을 넣어 문맥 벡터(context vector) 계산\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(src_tensor)\n",
        "\n",
        "    # 처음에는 <sos> 토큰 하나만 가지고 있도록 하기\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    for i in range(max_len):\n",
        "        # 이전에 출력한 단어가 현재 단어로 입력될 수 있도록\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
        "\n",
        "        pred_token = output.argmax(1).item()\n",
        "        trg_indexes.append(pred_token) # 출력 문장에 더하기\n",
        "\n",
        "        # <eos>를 만나는 순간 끝\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "\n",
        "    # 각 출력 단어 인덱스를 실제 단어로 변환\n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "\n",
        "    # 첫 번째 <sos>는 제외하고 출력 문장 반환\n",
        "    return trg_tokens[1:]"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCqSTEtyp9CU",
        "outputId": "7aa3ae9d-a5b9-4fbd-b299-2d8014e01ef6"
      },
      "source": [
        "example_idx = 10\n",
        "\n",
        "src = vars(test_data.examples[example_idx])['src']\n",
        "trg = vars(test_data.examples[example_idx])['trg']\n",
        "\n",
        "print(f'소스 문장: {src}')\n",
        "print(f'타겟 문장: {trg}')\n",
        "print(\"모델 출력 결과:\", \" \".join(translate_sentence(src, SRC, TRG, model, device)))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "소스 문장: ['.', 'freien', 'im', 'tag', 'schönen', 'einen', 'genießen', 'sohn', 'kleiner', 'ihr', 'und', 'mutter', 'eine']\n",
            "타겟 문장: ['a', 'mother', 'and', 'her', 'young', 'song', 'enjoying', 'a', 'beautiful', 'day', 'outside', '.']\n",
            "전체 소스 토큰: ['<SOS>', '.', 'freien', 'im', 'tag', 'schönen', 'einen', 'genießen', 'sohn', 'kleiner', 'ihr', 'und', 'mutter', 'eine', '<EOS>']\n",
            "소스 문장 인덱스: [2, 4, 88, 20, 200, 780, 19, 565, 624, 70, 134, 10, 364, 8, 3]\n",
            "모델 출력 결과: a young and her son are in the water in the background . <EOS>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhN-dPPrqBzG",
        "outputId": "22d9ae5d-7612-40b1-fd50-3cccb3fc3ab5"
      },
      "source": [
        "src = tokenize_de(\"Guten Abend.\")\n",
        "\n",
        "print(f'소스 문장: {src}')\n",
        "print(\"모델 출력 결과:\", \" \".join(translate_sentence(src, SRC, TRG, model, device)))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "소스 문장: ['.', 'Abend', 'Guten']\n",
            "전체 소스 토큰: ['<SOS>', '.', 'abend', 'guten', '<EOS>']\n",
            "소스 문장 인덱스: [2, 4, 1163, 3799, 3]\n",
            "모델 출력 결과: a <unk> . . <EOS>\n"
          ]
        }
      ]
    }
  ]
}